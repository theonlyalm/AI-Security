### üõ°Ô∏è AI Cloud Breach: The "Wiz" & "Hugging Face" Vulnerability

---

* **Target:** Hugging Face (The world's largest AI Model Hub)
* **Criticality:** High (Potential for Supply Chain Attack & Data Leakage)

---

### üìñ Executive Summary

In April 2024, Wiz Research revealed a startling reality about AI-as-a-Service (AIaaS): **Shared infrastructure is a ticking time bomb if not isolated perfectly.** Researchers demonstrated that by uploading a malicious AI model to **Hugging Face**, they could break out of their "tenant" sandbox, access the underlying cloud infrastructure (AWS EKS), and potentially steal the data and models of other customers. This wasn't just a bug; it was an architectural flaw in how AI services handle "untrusted" code (models).

---

### üõ†Ô∏è Technical Walkthrough: The Two Attack Paths

Wiz identified two distinct ways to compromise the platform, both relying on the fact that AI models and applications are effectively **executable code**.

#### **1. The Inference API Takeover (The "Pickle" Problem)**

This attack focused on the service that lets users "preview" models.

* **The Trojan Horse:** The researchers uploaded a PyTorch model using the **Pickle** format. Pickle is notorious in security circles because it allows arbitrary code execution upon loading.
* **The Breach:** When Hugging Face's servers loaded the model to generate a preview, the malicious Pickle file executed a "reverse shell," giving the researchers control over the server.
* **The Escalation (IMDS Abuse):** Once inside the server (a Kubernetes Pod), they queried the AWS **Instance Metadata Service (IMDS)**.
* *Result:* They stole the identity of the Kubernetes Node itself. This granted them broad permissions to view secrets and potentially access other customers' pods.



#### **2. The "Spaces" Supply Chain Attack**

"Spaces" is a feature where users can host AI apps (demos) using Docker containers.

* **The Flaw:** Users can submit a `Dockerfile` to define their app. Wiz found that by using the `RUN` command (which executes during the *build* phase) instead of `CMD` (which runs at *start*), they could execute code inside Hugging Face's build infrastructure.
* **The Critical Impact:** From the build server, they found they had **write access** to Hugging Face's internal **Container Registry**.
* *Scenario:* An attacker could have overwritten the "base images" used by *other* customers. The next time a legitimate customer built their app, they would unknowingly pull the attacker's malware‚Äîa classic **Supply Chain Attack**.



---

### üö® Why This Matters (The "Toxic Combination")

The research highlighted a "toxic combination" of risks unique to the AI era:

1. **Models = Code:** We often treat AI models like static files (like JPEGs), but they are actually programs. Downloading a random model is as dangerous as running a random `.exe`.
2. **Multitenancy Risks:** AI requires expensive GPUs. To make it affordable, providers pack many customers onto shared GPU clusters. If isolation fails (as it did here), one malicious customer can impact everyone else.
3. **Pickle Persistence:** Despite known risks, the AI industry still relies heavily on the insecurity-by-design "Pickle" format, making it hard to secure pipelines completely.

---

### ‚úÖ Remediation & Fixes

Hugging Face and Wiz collaborated to patch these holes before publication:

* **IMDSv2:** Hugging Face enforced **IMDSv2** with a "Hop Limit," effectively blocking pods from stealing the Node's cloud identity.
* **Registry Scoping:** The internal container registry was reconfigured to ensure customers could only access their own images, not the shared repository.
* **Sandboxing:** Enhanced sandboxing for untrusted models to prevent them from accessing the host network or sensitive environment variables.

